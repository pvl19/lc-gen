{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b590b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train hierarchical RNN (minLSTM/minGRU) autoencoder on light curve time series.\n",
    "\n",
    "Uses parallelizable RNN variants from \"Were RNNs All We Needed?\" (arXiv:2410.01201)\n",
    "Supports training with block masking and time-aware positional encoding.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('/Users/philvanlane/Documents/lc_ae/')\n",
    "from src.lcgen.models.rnn import HierarchicalRNN, RNNConfig\n",
    "from src.lcgen.data.masking import dynamic_block_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05727630",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/real_lightcurves/pk_star_sector_lc_cf.pickle\", \"rb\") as file:\n",
    "    lc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52b6c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sample light curve and sector\n",
    "sample = lc['405461319_42']\n",
    "\n",
    "# Pre-process time\n",
    "time = sample['time']\n",
    "sector_time = (time - time[0])\n",
    "\n",
    "# Pre-process flux\n",
    "flux = np.array(sample['flux'].data)\n",
    "mask = (np.isfinite(flux) & np.isfinite(time))\n",
    "flux = flux[mask]\n",
    "sector_time = sector_time[mask]\n",
    "flux_norm = (flux - np.nanmean(flux)) / np.nanstd(flux)\n",
    "\n",
    "# Pre-process flux error\n",
    "flux_err = np.array(sample['flux_err'].data)\n",
    "med_flux_error = np.nanmedian(flux_err)\n",
    "flux_err = flux_err[mask]\n",
    "flux_err = np.nan_to_num(flux_err, nan=med_flux_error, posinf=med_flux_error, neginf=med_flux_error)\n",
    "\n",
    "\n",
    "# Metadata\n",
    "metadata = {\n",
    "    'tic': sample['TIC_ID'],\n",
    "    'sector': sample['sector'],\n",
    "    'duration': sector_time[-1] - sector_time[0],\n",
    "    'med_flux_error': med_flux_error,\n",
    "    'n_points': len(flux),\n",
    "    'mean_flux': np.mean(flux),\n",
    "    'std_flux': np.std(flux),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e20952b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7911"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeff8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing light curve 0\n",
      "Processing light curve 100\n",
      "Processing light curve 200\n",
      "Processing light curve 300\n",
      "Processing light curve 400\n",
      "Processing light curve 500\n",
      "Processing light curve 600\n",
      "Processing light curve 700\n",
      "Processing light curve 800\n",
      "Processing light curve 900\n",
      "Processing light curve 1000\n",
      "Processing light curve 1100\n",
      "Processing light curve 1200\n",
      "Processing light curve 1300\n",
      "Processing light curve 1400\n",
      "Processing light curve 1500\n",
      "Processing light curve 1600\n",
      "Processing light curve 1700\n",
      "Processing light curve 1800\n",
      "Processing light curve 1900\n",
      "Processing light curve 2000\n",
      "Processing light curve 2100\n",
      "Processing light curve 2200\n",
      "Processing light curve 2300\n",
      "Processing light curve 2400\n",
      "Processing light curve 2500\n",
      "Processing light curve 2600\n",
      "Processing light curve 2700\n",
      "Processing light curve 2800\n",
      "Processing light curve 2900\n",
      "Processing light curve 3000\n",
      "Processing light curve 3100\n",
      "Processing light curve 3200\n",
      "Processing light curve 3300\n",
      "Processing light curve 3400\n",
      "Processing light curve 3500\n",
      "Processing light curve 3600\n",
      "Processing light curve 3700\n",
      "Processing light curve 3800\n",
      "Processing light curve 3900\n",
      "Processing light curve 4000\n",
      "Processing light curve 4100\n",
      "Processing light curve 4200\n",
      "Processing light curve 4300\n",
      "Processing light curve 4400\n",
      "Processing light curve 4500\n",
      "Processing light curve 4600\n",
      "Processing light curve 4700\n",
      "Processing light curve 4800\n",
      "Processing light curve 4900\n",
      "Processing light curve 5000\n",
      "Processing light curve 5100\n",
      "Processing light curve 5200\n",
      "Processing light curve 5300\n",
      "Processing light curve 5400\n",
      "Processing light curve 5500\n",
      "Processing light curve 5600\n",
      "Processing light curve 5700\n",
      "Processing light curve 5800\n",
      "Processing light curve 5900\n",
      "Processing light curve 6000\n",
      "Processing light curve 6100\n",
      "Processing light curve 6200\n",
      "Processing light curve 6300\n",
      "Processing light curve 6400\n",
      "Processing light curve 6500\n",
      "Processing light curve 6600\n",
      "Processing light curve 6700\n",
      "Processing light curve 6800\n",
      "Processing light curve 6900\n",
      "Processing light curve 7000\n",
      "Processing light curve 7100\n",
      "Processing light curve 7200\n",
      "Processing light curve 7300\n",
      "Processing light curve 7400\n",
      "Processing light curve 7500\n",
      "Processing light curve 7600\n",
      "Processing light curve 7700\n",
      "Processing light curve 7800\n",
      "Processing light curve 7900\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "fluxes = []\n",
    "flux_errs = []\n",
    "metadatas = []\n",
    "\n",
    "for i,k in enumerate(lc.keys()):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processing light curve {i}\")\n",
    "    # Use sample light curve and sector\n",
    "    sample = lc[k]\n",
    "\n",
    "    # Pre-process time\n",
    "    time = sample['time']\n",
    "    sector_time = (time - time[0])\n",
    "\n",
    "    # Pre-process flux\n",
    "    flux = np.array(sample['flux'].data)\n",
    "    mask = (np.isfinite(flux) & np.isfinite(time))\n",
    "    flux = flux[mask]\n",
    "    sector_time = sector_time[mask]\n",
    "    flux_norm = (flux - np.nanmean(flux)) / np.nanstd(flux)\n",
    "\n",
    "    # Pre-process flux error\n",
    "    flux_err = np.array(sample['flux_err'].data)\n",
    "    med_flux_error = np.nanmedian(flux_err)\n",
    "    flux_err = flux_err[mask]\n",
    "    flux_err = np.nan_to_num(flux_err, nan=med_flux_error, posinf=med_flux_error, neginf=med_flux_error)\n",
    "\n",
    "\n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        'tic': sample['TIC_ID'],\n",
    "        'sector': sample['sector'],\n",
    "        'duration': sector_time[-1] - sector_time[0],\n",
    "        'med_flux_error': med_flux_error,\n",
    "        'n_points': len(flux),\n",
    "        'mean_flux': np.nanmean(flux),\n",
    "        'std_flux': np.nanstd(flux),\n",
    "    }\n",
    "    times.append(sector_time)\n",
    "    fluxes.append(flux_norm)\n",
    "    flux_errs.append(flux_err)\n",
    "    metadatas.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a644397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "    'times': times,\n",
    "    'fluxes': fluxes,\n",
    "    'flux_errs': flux_errs,\n",
    "    'metadatas': metadatas,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ca3336e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15150"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict['fluxes'][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "04c46fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/real_lightcurves/star_sector_lc_formatted.pickle\", \"wb\") as file:\n",
    "    pickle.dump(dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f5234de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13081,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flux_err.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c85c883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['TIC_ID', 'sector', 'time', 'time_adj', 'flux', 'flux_err', 'flux_norm', 'flux_mean', 'asinh_mean', 'norm_asinh_mean', 'time_norm', 'flux_norm_standard', 'flux_err_norm_standard', 'flux_norm_absTmag', 'flux_err_norm_absTmag'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fdffe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405461319"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = {\n",
    "    'tic': sample['TIC_ID'],\n",
    "    'sector': sample['sector'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc35cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['TIC_ID', 'sector', 'time', 'time_adj', 'flux', 'flux_err', 'flux_norm', 'flux_mean', 'asinh_mean', 'norm_asinh_mean', 'time_norm', 'flux_norm_standard', 'flux_err_norm_standard', 'flux_norm_absTmag', 'flux_err_norm_absTmag'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc['405461319_42'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2159860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/mock_lightcurves/mock_lightcurves.pkl', 'rb') as f:\n",
    "    mock_lc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c528d701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['times', 'fluxes', 'flux_errs', 'metadatas'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_lc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6825b355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 3.25398265e-01, 6.50796530e-01, 9.76194795e-01,\n",
       "       1.30159306e+00, 1.62699132e+00, 1.95238959e+00, 2.60318612e+00,\n",
       "       3.25398265e+00, 3.57938091e+00, 3.90477918e+00, 4.23017744e+00,\n",
       "       4.55557571e+00, 4.88097397e+00, 5.20637224e+00, 5.85716877e+00,\n",
       "       6.18256703e+00, 6.50796530e+00, 6.83336356e+00, 7.48416009e+00,\n",
       "       8.13495662e+00, 8.46035489e+00, 8.78575315e+00, 9.43654968e+00,\n",
       "       9.76194795e+00, 1.00873462e+01, 1.04127445e+01, 1.07381427e+01,\n",
       "       1.10635410e+01, 1.13889393e+01, 1.17143375e+01, 1.20397358e+01,\n",
       "       1.23651341e+01, 1.26905323e+01, 1.30159306e+01, 1.33413289e+01,\n",
       "       1.36667271e+01, 1.39921254e+01, 1.43175237e+01, 1.46429219e+01,\n",
       "       1.49683202e+01, 1.52937185e+01, 1.59445150e+01, 1.62699132e+01,\n",
       "       1.65953115e+01, 1.69207098e+01, 1.75715063e+01, 1.78969046e+01,\n",
       "       1.82223028e+01, 1.85477011e+01, 1.91984976e+01, 1.95238959e+01,\n",
       "       1.98492942e+01, 2.01746924e+01, 2.05000907e+01, 2.08254890e+01,\n",
       "       2.11508872e+01, 2.14762855e+01, 2.18016838e+01, 2.21270820e+01,\n",
       "       2.24524803e+01, 2.27778785e+01, 2.31032768e+01, 2.37540733e+01,\n",
       "       2.47302681e+01, 2.50556664e+01, 2.53810647e+01, 2.57064629e+01,\n",
       "       2.60318612e+01, 2.63572595e+01, 2.66826577e+01, 2.70080560e+01,\n",
       "       2.73334543e+01, 2.83096491e+01, 2.86350473e+01, 2.89604456e+01,\n",
       "       2.92858438e+01, 2.96112421e+01, 2.99366404e+01, 3.02620386e+01,\n",
       "       3.05874369e+01, 3.12382334e+01, 3.15636317e+01, 3.18890300e+01,\n",
       "       3.22144282e+01, 3.25398265e+01, 3.28652248e+01, 3.31906230e+01,\n",
       "       3.38414196e+01, 3.41668178e+01, 3.48176144e+01, 3.51430126e+01,\n",
       "       3.54684109e+01, 3.57938091e+01, 3.61192074e+01, 3.64446057e+01,\n",
       "       3.70954022e+01, 3.74208005e+01, 3.77461987e+01, 3.83969953e+01,\n",
       "       3.87223935e+01, 3.90477918e+01, 3.93731901e+01, 3.96985883e+01,\n",
       "       4.00239866e+01, 4.03493849e+01, 4.06747831e+01, 4.10001814e+01,\n",
       "       4.13255797e+01, 4.16509779e+01, 4.19763762e+01, 4.23017744e+01,\n",
       "       4.26271727e+01, 4.29525710e+01, 4.32779692e+01, 4.36033675e+01,\n",
       "       4.39287658e+01, 4.42541640e+01, 4.45795623e+01, 4.49049606e+01,\n",
       "       4.55557571e+01, 4.62065536e+01, 4.65319519e+01, 4.68573502e+01,\n",
       "       4.71827484e+01, 4.75081467e+01, 4.78335450e+01, 4.81589432e+01,\n",
       "       4.84843415e+01, 4.88097397e+01, 4.91351380e+01, 4.94605363e+01,\n",
       "       4.97859345e+01, 5.01113328e+01, 5.04367311e+01, 5.10875276e+01,\n",
       "       5.14129259e+01, 5.17383241e+01, 5.20637224e+01, 5.23891207e+01,\n",
       "       5.27145189e+01, 5.33653155e+01, 5.36907137e+01, 5.40161120e+01,\n",
       "       5.43415103e+01, 5.46669085e+01, 5.49923068e+01, 5.56431033e+01,\n",
       "       5.59685016e+01, 5.62938998e+01, 5.66192981e+01, 5.69446964e+01,\n",
       "       5.72700946e+01, 5.75954929e+01, 5.79208912e+01, 5.82462894e+01,\n",
       "       5.85716877e+01, 5.92224842e+01, 5.95478825e+01, 6.05240773e+01,\n",
       "       6.11748738e+01, 6.15002721e+01, 6.18256703e+01, 6.28018651e+01,\n",
       "       6.41034582e+01, 6.44288565e+01, 6.47542547e+01, 6.50796530e+01,\n",
       "       6.60558478e+01, 6.63812461e+01, 6.67066443e+01, 6.70320426e+01,\n",
       "       6.76828391e+01, 6.80082374e+01, 6.83336356e+01, 6.86590339e+01,\n",
       "       6.89844322e+01, 6.93098304e+01, 6.96352287e+01, 6.99606270e+01,\n",
       "       7.02860252e+01, 7.06114235e+01, 7.09368218e+01, 7.12622200e+01,\n",
       "       7.15876183e+01, 7.22384148e+01, 7.28892114e+01, 7.32146096e+01,\n",
       "       7.35400079e+01, 7.38654062e+01, 7.45162027e+01, 7.48416009e+01,\n",
       "       7.51669992e+01, 7.54923975e+01, 7.58177957e+01, 7.64685923e+01,\n",
       "       7.67939905e+01, 7.74447871e+01, 7.80955836e+01, 7.84209819e+01,\n",
       "       7.87463801e+01, 7.93971767e+01, 7.97225749e+01, 8.00479732e+01,\n",
       "       8.03733715e+01, 8.06987697e+01, 8.10241680e+01, 8.13495662e+01,\n",
       "       8.16749645e+01, 8.23257610e+01, 8.26511593e+01, 8.29765576e+01,\n",
       "       8.33019558e+01, 8.36273541e+01, 8.39527524e+01, 8.42781506e+01,\n",
       "       8.49289472e+01, 8.52543454e+01, 8.55797437e+01, 8.59051420e+01,\n",
       "       8.62305402e+01, 8.65559385e+01, 8.68813368e+01, 8.72067350e+01,\n",
       "       8.75321333e+01, 8.78575315e+01, 8.81829298e+01, 8.85083281e+01,\n",
       "       8.88337263e+01, 8.91591246e+01, 8.98099211e+01, 9.01353194e+01,\n",
       "       9.04607177e+01, 9.07861159e+01, 9.11115142e+01, 9.14369125e+01,\n",
       "       9.20877090e+01, 9.27385055e+01, 9.30639038e+01, 9.33893021e+01,\n",
       "       9.37147003e+01, 9.40400986e+01, 9.43654968e+01, 9.46908951e+01,\n",
       "       9.53416916e+01, 9.56670899e+01, 9.59924882e+01, 9.63178864e+01,\n",
       "       9.66432847e+01, 9.69686830e+01, 9.72940812e+01, 9.79448778e+01,\n",
       "       9.82702760e+01, 9.85956743e+01, 9.89210726e+01, 9.92464708e+01,\n",
       "       9.95718691e+01, 9.98972674e+01, 1.00222666e+02, 1.00548064e+02,\n",
       "       1.00873462e+02, 1.01198860e+02, 1.01849657e+02, 1.02175055e+02,\n",
       "       1.02500453e+02, 1.02825852e+02, 1.03476648e+02, 1.03802047e+02,\n",
       "       1.04127445e+02, 1.04452843e+02, 1.04778241e+02, 1.05103640e+02,\n",
       "       1.05429038e+02, 1.05754436e+02, 1.06079834e+02, 1.06405233e+02,\n",
       "       1.06730631e+02, 1.07056029e+02, 1.07381427e+02, 1.08032224e+02,\n",
       "       1.08357622e+02, 1.08683021e+02, 1.09008419e+02, 1.09333817e+02,\n",
       "       1.09659215e+02, 1.09984614e+02, 1.10310012e+02, 1.11286207e+02,\n",
       "       1.11611605e+02, 1.12262401e+02, 1.12587800e+02, 1.12913198e+02,\n",
       "       1.13238596e+02, 1.14540189e+02, 1.14865588e+02, 1.15190986e+02,\n",
       "       1.15516384e+02, 1.15841782e+02, 1.16167181e+02, 1.16492579e+02,\n",
       "       1.16817977e+02, 1.17143375e+02, 1.17468774e+02, 1.17794172e+02,\n",
       "       1.18119570e+02, 1.18444968e+02, 1.18770367e+02, 1.19095765e+02,\n",
       "       1.19421163e+02, 1.19746562e+02, 1.20071960e+02, 1.20397358e+02,\n",
       "       1.20722756e+02, 1.21048155e+02, 1.21373553e+02, 1.21698951e+02,\n",
       "       1.22024349e+02, 1.22349748e+02, 1.23000544e+02, 1.23325942e+02,\n",
       "       1.23976739e+02, 1.24302137e+02, 1.24627535e+02, 1.24952934e+02,\n",
       "       1.25278332e+02, 1.25603730e+02, 1.25929129e+02, 1.26254527e+02,\n",
       "       1.26905323e+02, 1.27230722e+02, 1.27556120e+02, 1.27881518e+02,\n",
       "       1.28206916e+02, 1.28532315e+02, 1.28857713e+02, 1.29183111e+02,\n",
       "       1.29508509e+02, 1.29833908e+02, 1.30159306e+02, 1.30484704e+02,\n",
       "       1.31460899e+02, 1.32111696e+02, 1.32437094e+02, 1.32762492e+02,\n",
       "       1.33087890e+02, 1.33413289e+02, 1.33738687e+02, 1.34064085e+02,\n",
       "       1.34389483e+02, 1.35040280e+02, 1.35365678e+02, 1.36667271e+02,\n",
       "       1.36992670e+02, 1.37318068e+02, 1.37643466e+02, 1.38294263e+02,\n",
       "       1.38619661e+02, 1.39270457e+02, 1.39595856e+02, 1.39921254e+02,\n",
       "       1.40246652e+02, 1.40897449e+02, 1.41222847e+02, 1.41548245e+02,\n",
       "       1.41873644e+02, 1.42849838e+02, 1.43500635e+02, 1.43826033e+02,\n",
       "       1.44151431e+02, 1.44476830e+02, 1.44802228e+02, 1.45127626e+02,\n",
       "       1.45453024e+02, 1.45778423e+02, 1.46103821e+02, 1.46429219e+02,\n",
       "       1.46754618e+02, 1.47080016e+02, 1.47405414e+02, 1.47730812e+02,\n",
       "       1.48056211e+02, 1.48381609e+02, 1.48707007e+02, 1.49032405e+02,\n",
       "       1.49357804e+02, 1.49683202e+02, 1.50008600e+02, 1.50333998e+02,\n",
       "       1.50659397e+02, 1.50984795e+02, 1.51635591e+02, 1.51960990e+02,\n",
       "       1.52286388e+02, 1.53262583e+02, 1.53587981e+02, 1.53913379e+02,\n",
       "       1.54238778e+02, 1.54564176e+02, 1.54889574e+02, 1.55540371e+02,\n",
       "       1.55865769e+02, 1.56191167e+02, 1.57167362e+02, 1.57492760e+02,\n",
       "       1.57818159e+02, 1.58143557e+02, 1.58468955e+02, 1.58794353e+02,\n",
       "       1.59119752e+02, 1.59445150e+02, 1.59770548e+02, 1.60095946e+02,\n",
       "       1.60421345e+02, 1.60746743e+02, 1.61072141e+02, 1.61397539e+02,\n",
       "       1.61722938e+02, 1.62048336e+02, 1.62373734e+02, 1.62699132e+02,\n",
       "       1.63024531e+02, 1.63675327e+02, 1.64000726e+02, 1.64651522e+02,\n",
       "       1.65302319e+02, 1.65627717e+02, 1.65953115e+02, 1.66278513e+02,\n",
       "       1.66603912e+02, 1.67254708e+02, 1.67580106e+02, 1.67905505e+02,\n",
       "       3.65000000e+02, 3.65334820e+02, 3.65669641e+02, 3.66004461e+02,\n",
       "       3.66339281e+02, 3.66674102e+02, 3.67008922e+02, 3.67343742e+02,\n",
       "       3.68013383e+02, 3.68348203e+02, 3.68683024e+02, 3.69017844e+02,\n",
       "       3.69352664e+02, 3.69687484e+02, 3.70022305e+02, 3.70357125e+02,\n",
       "       3.70691945e+02, 3.71361586e+02, 3.71696406e+02, 3.72031227e+02,\n",
       "       3.72366047e+02, 3.72700867e+02, 3.73035688e+02, 3.73370508e+02,\n",
       "       3.73705328e+02, 3.74040149e+02, 3.74709789e+02, 3.75044610e+02,\n",
       "       3.75379430e+02, 3.75714250e+02, 3.76049071e+02, 3.76383891e+02,\n",
       "       3.76718711e+02, 3.77053532e+02, 3.77723172e+02, 3.78392813e+02,\n",
       "       3.78727633e+02, 3.79062453e+02, 3.79397274e+02])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_lc['times'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c16476",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/real_lightcurves/star_day_timeseries.pickle', 'rb') as f:\n",
    "    lc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af886b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tic', 'day', 'time_d_padded', 'flux_d_padded', 'flux_err_d_padded', 'norm_mean', 'norm_std'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc['384984325_2902'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ad151d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc_type': 'multiperiodic',\n",
       " 'sampling_strategy': 'astronomical',\n",
       " 'duration': 379.3972738151323,\n",
       " 'n_points': 471,\n",
       " 'snr': 17.302907326988453,\n",
       " 'offset': 10.40237733344247,\n",
       " 'noise_std': 0.07874219299524557,\n",
       " 'periods': array([79.44703621, 53.82900147, 38.08426937]),\n",
       " 'amplitudes': array([1.33483645, 1.04650404, 0.77585134]),\n",
       " 'phases': array([5.96601403, 0.92408993, 5.82192175])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc['metadatas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b20286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input INPUT] [--output_dir OUTPUT_DIR]\n",
      "                             [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                             [--lr LR]\n",
      "                             [--encoder_dims ENCODER_DIMS [ENCODER_DIMS ...]]\n",
      "                             [--rnn_type {minlstm,minGRU}]\n",
      "                             [--num_layers NUM_LAYERS]\n",
      "                             [--num_layers_per_level NUM_LAYERS_PER_LEVEL]\n",
      "                             [--mask_ratio MASK_RATIO]\n",
      "                             [--block_size BLOCK_SIZE] [--val_split VAL_SPLIT]\n",
      "                             [--seed SEED] [--device DEVICE]\n",
      "                             [--save_every SAVE_EVERY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/philvanlane/Library/Jupyter/runtime/kernel-v3df2f76f03ef5fc5e9231dfcce9065f09e4b49105.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philvanlane/opt/anaconda3/envs/default-py312/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3680: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class LightCurveDataset(Dataset):\n",
    "    \"\"\"Dataset for light curve time series with timestamps.\"\"\"\n",
    "\n",
    "    def __init__(self, flux, timestamps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            flux: Numpy array of shape (N, seq_len)\n",
    "            timestamps: Numpy array of shape (N, seq_len)\n",
    "        \"\"\"\n",
    "        self.flux = torch.from_numpy(flux).float()\n",
    "        self.timestamps = torch.from_numpy(timestamps).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.flux)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return flux and timestamps - masking will be applied in collate_fn\n",
    "        return {\n",
    "            'flux': self.flux[idx],\n",
    "            'time': self.timestamps[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_with_masking(batch, min_block_size=1, max_block_size=None,\n",
    "                         min_mask_ratio=0.1, max_mask_ratio=0.9):\n",
    "    \"\"\"\n",
    "    Custom collate function that applies masking to light curves.\n",
    "\n",
    "    Creates input as [masked_flux, mask_indicator] for the transformer.\n",
    "    \"\"\"\n",
    "    # Extract flux and timestamps\n",
    "    batch_flux = torch.stack([item['flux'] for item in batch], dim=0)  # [batch_size, seq_len]\n",
    "    batch_time = torch.stack([item['time'] for item in batch], dim=0)  # [batch_size, seq_len]\n",
    "\n",
    "    batch_size, seq_len = batch_flux.shape\n",
    "\n",
    "    if max_block_size is None:\n",
    "        max_block_size = seq_len // 2\n",
    "\n",
    "    # Apply masking per sample\n",
    "    batch_inputs = []\n",
    "    batch_targets = []\n",
    "    batch_masks = []\n",
    "    batch_block_sizes = []\n",
    "    batch_mask_ratios = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        flux = batch_flux[i]\n",
    "\n",
    "        # Apply dynamic block masking to flux\n",
    "        flux_masked, mask, block_size, mask_ratio = dynamic_block_mask(\n",
    "            flux,\n",
    "            min_block_size=min_block_size,\n",
    "            max_block_size=max_block_size,\n",
    "            min_mask_ratio=min_mask_ratio,\n",
    "            max_mask_ratio=max_mask_ratio\n",
    "        )\n",
    "\n",
    "        # Stack flux and mask as two channels: [masked_flux, mask_indicator]\n",
    "        # Shape: (seq_len, 2)\n",
    "        input_with_mask = torch.stack([flux_masked, mask.float()], dim=-1)\n",
    "\n",
    "        batch_inputs.append(input_with_mask)\n",
    "        batch_targets.append(flux)\n",
    "        batch_masks.append(mask)\n",
    "        batch_block_sizes.append(block_size)\n",
    "        batch_mask_ratios.append(mask_ratio)\n",
    "\n",
    "    return {\n",
    "        'input': torch.stack(batch_inputs),  # (batch, seq_len, 2)\n",
    "        'target': torch.stack(batch_targets),  # (batch, seq_len)\n",
    "        'time': batch_time,  # (batch, seq_len)\n",
    "        'mask': torch.stack(batch_masks),  # (batch, seq_len)\n",
    "        'block_size': torch.tensor(batch_block_sizes),\n",
    "        'mask_ratio': torch.tensor(batch_mask_ratios)\n",
    "    }\n",
    "\n",
    "\n",
    "def load_lightcurve_data(hdf5_file):\n",
    "    \"\"\"\n",
    "    Load light curve data from HDF5 file.\n",
    "\n",
    "    Expected structure:\n",
    "        - 'flux': (N, seq_len) - normalized flux values\n",
    "        - 'time': (N, seq_len) - timestamps\n",
    "\n",
    "    Args:\n",
    "        hdf5_file: Path to HDF5 file\n",
    "\n",
    "    Returns:\n",
    "        flux: Numpy array of shape (N, seq_len)\n",
    "        timestamps: Numpy array of shape (N, seq_len)\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_file, 'r') as f:\n",
    "        flux = f['flux'][:]\n",
    "        timestamps = f['time'][:]\n",
    "        print(f\"Loaded flux: {flux.shape}, time: {timestamps.shape}\")\n",
    "\n",
    "        # Check for NaNs\n",
    "        n_nans_flux = np.sum(np.isnan(flux))\n",
    "        n_nans_time = np.sum(np.isnan(timestamps))\n",
    "        if n_nans_flux > 0:\n",
    "            print(f\"Warning: {n_nans_flux} NaN values found in flux\")\n",
    "            flux = np.nan_to_num(flux, nan=0.0)\n",
    "        if n_nans_time > 0:\n",
    "            print(f\"Warning: {n_nans_time} NaN values found in timestamps\")\n",
    "            timestamps = np.nan_to_num(timestamps, nan=0.0)\n",
    "\n",
    "        return flux, timestamps\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, scheduler=None):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_masked_loss = 0\n",
    "    total_unmasked_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        x_input = batch['input'].to(device)  # (batch, seq_len, 2) [flux, mask]\n",
    "        x_target = batch['target'].to(device)  # (batch, seq_len)\n",
    "        timestamps = batch['time'].to(device)  # (batch, seq_len)\n",
    "        mask = batch['mask'].to(device)  # (batch, seq_len)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_input, timestamps)\n",
    "        x_recon = output['reconstructed'].squeeze(-1)  # (batch, seq_len)\n",
    "\n",
    "        # Compute loss over ALL regions (both masked and unmasked)\n",
    "        loss = criterion(x_recon, x_target)\n",
    "\n",
    "        # Also track masked vs unmasked separately for monitoring\n",
    "        masked_loss = criterion(x_recon[mask], x_target[mask])\n",
    "        unmasked_loss = criterion(x_recon[~mask], x_target[~mask])\n",
    "\n",
    "        # Backprop and optimize on full reconstruction loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step scheduler after each batch (for OneCycleLR)\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_masked_loss += masked_loss.item()\n",
    "        total_unmasked_loss += unmasked_loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return {\n",
    "        'total_loss': total_loss / n_batches,\n",
    "        'masked_loss': total_masked_loss / n_batches,\n",
    "        'unmasked_loss': total_unmasked_loss / n_batches\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_masked_loss = 0\n",
    "    total_unmasked_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_input = batch['input'].to(device)\n",
    "            x_target = batch['target'].to(device)\n",
    "            timestamps = batch['time'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(x_input, timestamps)\n",
    "            x_recon = output['reconstructed'].squeeze(-1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(x_recon, x_target)\n",
    "            masked_loss = criterion(x_recon[mask], x_target[mask])\n",
    "            unmasked_loss = criterion(x_recon[~mask], x_target[~mask])\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_masked_loss += masked_loss.item()\n",
    "            total_unmasked_loss += unmasked_loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "    return {\n",
    "        'total_loss': total_loss / n_batches,\n",
    "        'masked_loss': total_masked_loss / n_batches,\n",
    "        'unmasked_loss': total_unmasked_loss / n_batches\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_reconstruction_examples(model, dataloader, device, save_path, n_examples=4):\n",
    "    \"\"\"Plot reconstruction examples.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Get one batch\n",
    "    batch = next(iter(dataloader))\n",
    "    x_input = batch['input'].to(device)[:n_examples]\n",
    "    x_target = batch['target'].to(device)[:n_examples]\n",
    "    timestamps = batch['time'].to(device)[:n_examples]\n",
    "    mask = batch['mask'].to(device)[:n_examples]\n",
    "    block_sizes = batch['block_size'][:n_examples].numpy()\n",
    "    mask_ratios = batch['mask_ratio'][:n_examples].numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x_input, timestamps)\n",
    "        x_recon = output['reconstructed'].squeeze(-1)\n",
    "\n",
    "    # Move to CPU for plotting\n",
    "    x_target = x_target.cpu().numpy()\n",
    "    x_recon = x_recon.cpu().numpy()\n",
    "    timestamps_np = timestamps.cpu().numpy()\n",
    "    mask = mask.cpu().numpy()\n",
    "\n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(n_examples, 1, figsize=(12, 3*n_examples))\n",
    "    if n_examples == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        t = timestamps_np[i]\n",
    "        target = x_target[i]\n",
    "        recon = x_recon[i]\n",
    "        m = mask[i]\n",
    "        block_size = block_sizes[i]\n",
    "        mask_ratio = mask_ratios[i]\n",
    "\n",
    "        # Plot target and reconstruction\n",
    "        ax.plot(t, target, 'k-', alpha=0.6, label='Target', linewidth=1.5)\n",
    "        ax.plot(t, recon, 'b-', alpha=0.8, label='Reconstruction', linewidth=1.5)\n",
    "\n",
    "        # Highlight masked regions\n",
    "        if m.sum() > 0:\n",
    "            # Find contiguous masked regions\n",
    "            masked_indices = np.where(m)[0]\n",
    "            ax.scatter(t[masked_indices], recon[masked_indices],\n",
    "                      c='red', s=10, alpha=0.5, label='Masked regions')\n",
    "\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Flux')\n",
    "        ax.set_title(f'Example {i+1} (masked {mask_ratio*100:.1f}%, block size {block_size})')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"Saved reconstruction plot to {save_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Train hierarchical RNN (minLSTM/minGRU) autoencoder on light curves')\n",
    "    parser.add_argument('--input', type=str,\n",
    "                        default='data/mock_lightcurves/timeseries.h5',\n",
    "                        help='Path to HDF5 file with light curve time series')\n",
    "    parser.add_argument('--output_dir', type=str, default='models/rnn',\n",
    "                        help='Directory to save model checkpoints')\n",
    "    parser.add_argument('--epochs', type=int, default=50,\n",
    "                        help='Number of training epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='Batch size')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                        help='Learning rate')\n",
    "    parser.add_argument('--encoder_dims', type=int, nargs='+', default=[64, 128, 256, 512],\n",
    "                        help='Encoder dimensions for hierarchical levels')\n",
    "    parser.add_argument('--rnn_type', type=str, default='minlstm', choices=['minlstm', 'minGRU'],\n",
    "                        help='RNN cell type (minlstm or minGRU)')\n",
    "    parser.add_argument('--num_layers', type=int, default=4,\n",
    "                        help='Number of hierarchical levels')\n",
    "    parser.add_argument('--num_layers_per_level', type=int, default=2,\n",
    "                        help='RNN layers per hierarchy level')\n",
    "    parser.add_argument('--mask_ratio', type=float, default=0.5,\n",
    "                        help='Maximum masking ratio')\n",
    "    parser.add_argument('--block_size', type=int, default=32,\n",
    "                        help='Maximum block size for masking')\n",
    "    parser.add_argument('--val_split', type=float, default=0.15,\n",
    "                        help='Validation set fraction')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help='Random seed')\n",
    "    parser.add_argument('--device', type=str, default='auto',\n",
    "                        help='Device (cuda/cpu/auto)')\n",
    "    parser.add_argument('--save_every', type=int, default=10,\n",
    "                        help='Save checkpoint every N epochs')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # Device\n",
    "    if args.device == 'auto':\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    else:\n",
    "        device = torch.device(args.device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    print(f\"\\nLoading data from {args.input}...\")\n",
    "    flux, timestamps = load_lightcurve_data(args.input)\n",
    "\n",
    "    print(f\"\\nDataset statistics:\")\n",
    "    print(f\"  Shape: {flux.shape}\")\n",
    "    print(f\"  Flux range: [{np.min(flux):.3f}, {np.max(flux):.3f}]\")\n",
    "    print(f\"  Flux mean: {np.mean(flux):.3f}\")\n",
    "    print(f\"  Flux std: {np.std(flux):.3f}\")\n",
    "    print(f\"  Time range: [{np.min(timestamps):.3f}, {np.max(timestamps):.3f}]\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = LightCurveDataset(flux, timestamps)\n",
    "\n",
    "    # Split into train/val\n",
    "    val_size = int(len(dataset) * args.val_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(args.seed)\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDataset split:\")\n",
    "    print(f\"  Train: {train_size} samples\")\n",
    "    print(f\"  Val: {val_size} samples\")\n",
    "\n",
    "    # Create collate function with masking parameters\n",
    "    collate_fn = partial(\n",
    "        collate_with_masking,\n",
    "        min_block_size=1,\n",
    "        max_block_size=args.block_size,\n",
    "        min_mask_ratio=0.1,\n",
    "        max_mask_ratio=args.mask_ratio\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    seq_len = flux.shape[1]\n",
    "    print(f\"\\nCreating Hierarchical RNN ({args.rnn_type}) autoencoder...\")\n",
    "    config = RNNConfig(\n",
    "        input_dim=2,  # [flux, mask]\n",
    "        input_length=seq_len,\n",
    "        encoder_dims=args.encoder_dims,\n",
    "        rnn_type=args.rnn_type,\n",
    "        num_layers_per_level=args.num_layers_per_level,\n",
    "        dropout=0.0,  # No dropout\n",
    "        min_period=0.00278,\n",
    "        max_period=1640.0\n",
    "    )\n",
    "\n",
    "    model = HierarchicalRNN(config).to(device)\n",
    "\n",
    "    # Count parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model parameters: {n_params:,}\")\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)\n",
    "\n",
    "    # OneCycleLR scheduler\n",
    "    total_steps = len(train_loader) * args.epochs\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=args.lr,\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.1,\n",
    "        div_factor=1e2,\n",
    "        final_div_factor=1e2\n",
    "    )\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training loop\n",
    "    print(f\"\\nStarting training for {args.epochs} epochs...\")\n",
    "    max_block_display = args.block_size if args.block_size else \"seq_len//2\"\n",
    "    print(f\"Dynamic masking: block size [1, {max_block_display}], \"\n",
    "          f\"mask ratio [10%, {args.mask_ratio*100:.0f}%]\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train\n",
    "        train_metrics = train_epoch(model, train_loader, optimizer, criterion, device, scheduler)\n",
    "        train_losses.append(train_metrics['total_loss'])\n",
    "\n",
    "        # Validate\n",
    "        val_metrics = validate_epoch(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_metrics['total_loss'])\n",
    "\n",
    "        # Print progress (match MLP format)\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "        print(f\"  Train - Loss: {train_metrics['total_loss']:.6f} | \"\n",
    "              f\"Masked: {train_metrics['masked_loss']:.6f} | \"\n",
    "              f\"Unmasked: {train_metrics['unmasked_loss']:.6f}\")\n",
    "        print(f\"  Val   - Loss: {val_metrics['total_loss']:.6f} | \"\n",
    "              f\"Masked: {val_metrics['masked_loss']:.6f} | \"\n",
    "              f\"Unmasked: {val_metrics['unmasked_loss']:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['total_loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['total_loss']\n",
    "            checkpoint_path = output_dir / 'rnn_best.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_metrics['total_loss'],\n",
    "                'val_loss': best_val_loss,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  Saved best model to {checkpoint_path}\")\n",
    "\n",
    "        # Plot samples every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            plot_path = output_dir / f'rnn_recon_epoch{epoch+1}.png'\n",
    "            plot_reconstruction_examples(\n",
    "                model, val_loader, device, plot_path\n",
    "            )\n",
    "\n",
    "        # Save periodic checkpoints\n",
    "        if (epoch + 1) % args.save_every == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_metrics['total_loss'],\n",
    "                'config': config\n",
    "            }, output_dir / f'rnn_checkpoint_epoch{epoch+1}.pt')\n",
    "\n",
    "    # Save final model\n",
    "    final_path = output_dir / 'rnn_final.pt'\n",
    "    torch.save({\n",
    "        'epoch': args.epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'config': config\n",
    "    }, final_path)\n",
    "    print(f\"\\nSaved final model to {final_path}\")\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title(f'Hierarchical RNN ({args.rnn_type}) Training')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(output_dir / 'rnn_training_curve.png', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
